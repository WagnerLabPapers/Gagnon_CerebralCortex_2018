{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Localizer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as op\n",
    "import os as os\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nilearn.masking import compute_epi_mask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Nilearn for neuro-imaging-specific machine learning\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import image\n",
    "\n",
    "# Nibabel for general neuro-imaging tools\n",
    "import nibabel\n",
    "\n",
    "# Scikit-learn for machine learning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import LeaveOneLabelOut, LeavePLabelOut, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "import seaborn as sns\n",
    "sns.set(context=\"poster\", style=\"ticks\", font=\"Arial\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, np.round(cm[i, j], decimals=2),\n",
    "                 horizontalalignment=\"center\", size='xx-large',\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def plot_results(subj_info, cm_group, df_acc, df_auc=None, \n",
    "                 classes=['animal', 'face', 'fruitveg', 'tool', 'virtualtown']):\n",
    "    \n",
    "    if df_auc is not None:\n",
    "        data = df_auc.merge(subj_info)\n",
    "        print data.group.value_counts()/10\n",
    "    \n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm_group.mean(axis=0), classes=classes,\n",
    "                          title='Mean confusion matrix')\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.factorplot(x='category', y='accuracy', hue='classifier', aspect=2,\n",
    "                   units='subid', ci=68, data=df_acc, dodge=.1)\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.boxplot(x='category', y='accuracy', hue='classifier', data=df_acc)\n",
    "    sns.stripplot(x='category', y='accuracy', hue='classifier', jitter=True, \n",
    "                  data=df_acc)\n",
    "    \n",
    "    if df_auc is not None:\n",
    "        sns.factorplot(x='category', y='auc', hue='group', aspect=2,\n",
    "                       units='subid', ci=68, data=data, dodge=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up some colors for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palette = {'logreg': 'mediumseagreen',\n",
    "           'chance': 'darkgray',\n",
    "           'f1 score': 'teal'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# While debugging:\n",
    "%load_ext autoreload\n",
    "%aimport ap_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ap_classify import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up directory & file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoothing = 'unsmoothed'\n",
    "regspace = 'epi'\n",
    "design = 'localizer_cond_mvpa.csv' # onset file in lyman-style\n",
    "# design = 'localizer_subcat.csv' # onset file in lyman-style\n",
    "smoothing_fwhm = 0\n",
    "standardize = True\n",
    "tr = float(2) # in seconds\n",
    "tr_shift = 4.5 # seconds to shift forward by\n",
    "ts_type = 'raw' # raw or residual\n",
    "run_list = [7, 8]\n",
    "\n",
    "basedir = '/Volumes/group/awagner/sgagnon/AP'\n",
    "analydir = op.join(basedir, 'analysis/mvpa_raw')\n",
    "subjfile = op.join(analydir, 'notebooks/subj_info.csv')\n",
    "subj_info = pd.read_csv(subjfile)\n",
    "\n",
    "# Filepath templates\n",
    "if ts_type == 'raw':\n",
    "    tsfilename = 'timeseries_xfm.nii.gz'\n",
    "elif ts_type == 'residual':\n",
    "    tsfilename = 'res4d_xfm.nii.gz'\n",
    "tsfile = op.join(analydir, \"{subid}\", 'reg', regspace, \n",
    "                 smoothing, \"run_{run_id}\", tsfilename)\n",
    "func_maskfile = op.join(analydir, \"{subid}\", 'reg', regspace, \n",
    "                        smoothing, \"run_{run_id}\", 'functional_mask_xfm.nii.gz')\n",
    "maskfile = op.join(basedir, 'data', \"{subid}\", 'masks', \n",
    "                   \"{mask_name}.nii.gz\")\n",
    "meanfile = op.join(analydir, \"{subid}\", 'preproc',\n",
    "                   \"run_{run_id}\", 'mean_func.nii.gz')\n",
    "onsetfile = op.join(basedir, 'data', \"{subid}\", 'design', design)\n",
    "\n",
    "# Output templates\n",
    "outnifti = op.join(analydir, \"{subid}\", 'importance_maps')\n",
    "\n",
    "artifacts = op.join(analydir, '{subid}', 'preproc', 'run_{run}', 'artifacts.csv')\n",
    "\n",
    "# Combine paths into dictionary (facilitate passing i/o of funcs)\n",
    "paths = dict(tsfile=tsfile, func_maskfile=func_maskfile, \n",
    "             maskfile=maskfile, meanfile=meanfile, \n",
    "             onsetfile=onsetfile, outnifti=outnifti, \n",
    "             analydir=analydir, artifacts=artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create anatomical masks in native space from a cortical parcellation of the high-resolution T1\n",
    "image obtained for each participant using FreeSurfer and the resulting **bilateral\n",
    "inferior temporal cortex**, **fusiform gyrus**, and **parahippocampal gyrus** were combined to serve\n",
    "as the mask for MVPA classification (as in Zeithamova et al., 2012; *Neuron*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classification (training/testing on 3 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stress     22\n",
       "control    22\n",
       "Name: group, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_info.group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run localizer CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize some dataframes for storage\n",
    "df = pd.DataFrame(columns=['subid', 'mask_name', 'category', 'type', 'mean', 'sd'])\n",
    "df_acc = pd.DataFrame(columns=['subid', 'mask_name', 'category', 'classifier', 'accuracy', 'count'])\n",
    "df_proba = pd.DataFrame(columns=['subid', 'mask_name', 'true_category', 'guess_category', 'classifier', 'probability'])\n",
    "df_auc = pd.DataFrame()\n",
    "\n",
    "n_permutations=0 # or if want to run permutation test, e.g., 100\n",
    "if n_permutations > 0:\n",
    "    iter_list = list(np.arange(1, n_permutations + 1))\n",
    "    iter_list.insert(0, 'subid')\n",
    "    d_permute = pd.DataFrame(columns=iter_list)\n",
    "\n",
    "mask_type = 'mask' # functional mask, or anatomical mask defined w/mask_name\n",
    "mask_name = 'bilat-parahipp_fusi_inftemp_nohipp'\n",
    "# mask_name = 'bilat-fusi_inftemp_nohipp' # excluding parahipp to see if just that\n",
    "# mask_name = 'lh-inferiorparietal'\n",
    "cond_list = ['face', 'object', 'place']\n",
    "multi_class = 'ovr'\n",
    "pca_n = None #or None\n",
    "univariate_fsl_k=None # 1000 #or None\n",
    "\n",
    "# mask_type = 'func' # functional mask, or anatomical mask defined w/mask_name\n",
    "# mask_name = 'wholebrain'\n",
    "# cond_list = ['face', 'object', 'place']\n",
    "\n",
    "# Confusion matrix\n",
    "cm_group = np.zeros((1,len(cond_list), len(cond_list)))\n",
    "\n",
    "# Iterate through subjects\n",
    "for subid in subj_info.subid:\n",
    "    print subid\n",
    "    \n",
    "    onsetfile = paths['onsetfile']\n",
    "    # Get subj-specific data\n",
    "    X, run_labels, ev_labels, ev_trs, ev_onsets, func_masker = get_subj_data(subid, onsetfile, cond_list, paths, mask_type, mask_name,\n",
    "                                                                             smoothing_fwhm, standardize, tr, tr_shift, run_list, \n",
    "                                                                             shift_rest=True, filter_artifacts=True)\n",
    "    \n",
    "    cv = LeaveOneLabelOut(run_labels)\n",
    "    plot_validation_curve(X, ev_labels, cv)\n",
    "    \n",
    "    # Classification\n",
    "    if n_permutations > 0:\n",
    "        df, d_permute = calc_scores(df, subid, mask_name, X, ev_labels, run_labels, \n",
    "                                    n_permutations=n_permutations, d_permute=d_permute, \n",
    "                                    plot_permutation=True, multi_class=multi_class)\n",
    "    else:\n",
    "        df = calc_scores(df, subid, mask_name, X, ev_labels, run_labels, multi_class=multi_class)\n",
    "    \n",
    "    if multi_class == 'MLP':\n",
    "        df_acc, df_proba, cm_group = calc_acc_proba(df_acc, df_proba, subid, mask_name, X, ev_labels, \n",
    "                                                    run_labels, cv,conf_mat=True, multi_class=multi_class, \n",
    "                                                    cm_group=cm_group)\n",
    "    else:\n",
    "        df_acc, df_proba, df_auc, cm_group = calc_acc_proba(df_acc, df_proba, subid, mask_name, X, ev_labels, \n",
    "                                                            run_labels, cv,conf_mat=True, multi_class=multi_class, \n",
    "                                                            univariate_fsel_k=univariate_fsl_k, undersampling=True,\n",
    "                                                            pca_n=pca_n,\n",
    "                                                            cm_group=cm_group,\n",
    "                                                            compute_AUC=True, df_auc=df_auc,\n",
    "                                                            repeated_ttest_fsel=None)\n",
    "    \n",
    "#     # Create coef maps by training on all data, save niis and pngs to dir\n",
    "#     create_coef_maps(subid, X, ev_labels, func_masker, mask_name, paths, calc_A=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various possible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('output_ap/localizer_scores_filterart_raw_scalewithinrun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_acc.to_csv('output_ap/localizer_accuracy_raw.csv', index=False)\n",
    "df_proba.to_csv('output_ap/localizer_proba_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VTC - no hipp\n",
    "df_auc.to_csv('output_ap/localizer_vtcnohipp_auc_filterart_raw_scalewithinrun.csv')\n",
    "df_acc.to_csv('output_ap/localizer_vtcnohipp_accuracy_filterart_raw_scalewithinrun.csv')\n",
    "df_proba.to_csv('output_ap/localizer_vtcnohipp_proba_filterart_raw_scalewithinrun.csv')\n",
    "df.to_csv('output_ap/localizer_vtcnohipp_df_filterart_raw_scalewithinrun.csv')\n",
    "d_permute.to_csv('output_ap/localizer_vtcnohipp_permute_filterart_raw_scalewithinrun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whole hippocampus\n",
    "df_auc.to_csv('output_ap/localizer_bilat-hipp_auc_filterart_raw_scalewithinrun.csv')\n",
    "df_acc.to_csv('output_ap/localizer_bilat-hipp_accuracy_filterart_raw_scalewithinrun.csv')\n",
    "df_proba.to_csv('output_ap/localizer_bilat-hipp_proba_filterart_raw_scalewithinrun.csv')\n",
    "df.to_csv('output_ap/localizer_bilat-hipp_df_filterart_raw_scalewithinrun.csv')\n",
    "d_permute.to_csv('output_ap/localizer_bilat-hipp_permute_filterart_raw_scalewithinrun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inf parietal\n",
    "df_auc.to_csv('output_ap/localizer_inferiorparietal_auc_filterart_raw_scalewithinrun.csv')\n",
    "df_acc.to_csv('output_ap/localizer_inferiorparietal_accuracy_filterart_raw_scalewithinrun.csv')\n",
    "df_proba.to_csv('output_ap/localizer_inferiorparietal_proba_filterart_raw_scalewithinrun.csv')\n",
    "df.to_csv('output_ap/localizer_inferiorparietal_df_filterart_raw_scalewithinrun.csv')\n",
    "d_permute.to_csv('output_ap/localizer_inferiorparietal_permute_filterart_raw_scalewithinrun.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
